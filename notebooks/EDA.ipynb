{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "### 0.1 Case Study\n",
    "\n",
    "#### Scenario\n",
    "At BMW, we reimagine the future of mobility. Lets fast forward to 2030, and flying taxis are roaming above our cities, bringing people to\n",
    "their desired destination. You work for, Duoro Hawk a company that is pioneering the world's first large fleet of fully electric, self-piloting\n",
    "autonomous flying taxis. The company wants to deploy the first network of autonomous air taxis in the coming year. As part of our data\n",
    "science and enginering team, you are responsible for predicting the destination of our fleet of autonomous flying taxis based on the\n",
    "manned test flights that have been performed.\n",
    "\n",
    "#### About the Dataset\n",
    "A fictional dataset describing a complete year (from 01/07/2014 to 30/06/2014) of all the trajectories for all 442 of our flying taxis that\n",
    "were simulated in the city of Porto. Our autonomous fleet of taxis fly from a central ground station\n",
    "• There are three different types of rides: A) phone call-based, B) stand-based where people wait at a stand for their flying taxi or C) \n",
    "random place. For type A, we provide an anonymized ID, to represent the telephone call. Categories B and C refers to cases where the\n",
    "taxis were directly called by the customer.\n",
    "\n",
    "#### Dataset\n",
    "##### train.csv\n",
    "Each data sample corresponds to one completed trip. It contains a total of 9 (nine) features, described as follows:\n",
    "\n",
    "- TRIP_ID: (String) It contains an unique identifier for each trip;\n",
    "\n",
    "- CALL_TYPE: (char) It identifies the way used to demand this service. It may contain one of three possible values:\n",
    "     - ‘A’ if this trip was dispatched from the central;\n",
    "     - ‘B’ if this trip was demanded directly to a taxi driver on a specific stand;\n",
    "     - ‘C’ otherwise (i.e. a trip demanded on a random street).\n",
    "     \n",
    "- ORIGIN_CALL: (integer) It contains an unique identifier for each phone number which was used to demand, at least, one service. It identifies the trip’s customer if CALLTYPE=’A’. Otherwise, it assumes a NULL value;\n",
    "\n",
    "- ORIGINSTAND: (integer): It contains an unique identifier for the taxi stand. It identifies the starting point of the trip if CALLTYPE=’B’. Otherwise, it assumes a NULL value;\n",
    "\n",
    "- WEATHER: (String): Information on the weather that day, unique values include: Sunny, Rainy, Cloudy, Windy, and Foggy\n",
    "- TAXI_ID: (integer): It contains an unique identifier for the flying taxi that performed each trip;\n",
    "- TIMESTAMP: (integer) Unix Timestamp (in seconds). It identifies the trip’s start;\n",
    "- MISSING_DATA: (Boolean) It is FALSE when the GPS data stream is complete and TRUE whenever one (or more) locations are missing\n",
    "- POLYLINE: (String): It contains a list of GPS coordinates (i.e. WGS84 format) mapped as a string. The beginning and the end of the string are identified with brackets (i.e. [ and ], respectively). Each pair of coordinates is also identified by the same brackets as\n",
    "- [LONGITUDE, LATITUDE]. This list contains one pair of coordinates for each 15 seconds of trip. The last list item corresponds to the trip’s destination while the first one represents its start\n",
    "\n",
    "\n",
    "##### test.csv\n",
    "Personal records for the remaining one-third (~110) of the trips, to be used as test data. Your task is to predict the value of coordinates of the trip‘s destination\n",
    "\n",
    "##### sample_submission.csv \n",
    "A submission file in the correct format.\n",
    "- TripId - Id for each Tip in the test set\n",
    "- Longitude - the longitude of the destination of the flying taxi\n",
    "- Latitude – the latitude of the destination of the flying taxi\n",
    "\n",
    "The total travel time of the trip (the prediction target of this competition) is defined as the (number of points-1) x 15 seconds. For example, a trip with 101 data points in POLYLINE has a length of (101-1) * 15 = 1500 seconds. Some trips have missing data points in POLYLINE, indicated by MISSING_DATA column, and it is part of the challenge how you utilize this knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_sample_size(data, add_text=''):\n",
    "    print(f'{add_text} Size data samples {data.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (!) Due to multiple issues caused by the size of the train_df file, all files will be uploaded to a S3 location as parquet files and loaded from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('s3://think-tank-casestudy/raw_data/train_df.csv', header=0, index_col=False)\n",
    "train_data.to_parquet('s3://think-tank-casestudy/raw_data/train_data.parquet')\n",
    "\n",
    "test_data = pd.read_csv('data/test_df.csv',header=0, index_col=False)\n",
    "test_data.to_parquet('s3://think-tank-casestudy/raw_data/test_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_parquet('s3://think-tank-casestudy/raw_data/train_data.parquet')\n",
    "test_data = pd.read_parquet('s3://think-tank-casestudy/raw_data/test_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sampleSubmission.csv', header=0, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target prediction is longitude and latitude for each TRIP in test_data --> **MULTI REGRESSION OUTPUT**\n",
    "Relevant for modelling later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    1710670\n",
       "Name: DAY_TYPE, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.DAY_TYPE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    320\n",
       "Name: DAY_TYPE, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.DAY_TYPE.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAY_TYPE is not mentioned in list of attributes and attribute is uniformly distributed for both train and test data --> no information gain and can therefore be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.copy().drop(train_data.columns[0],axis=1).drop(['DAY_TYPE'],axis=1)\n",
    "test_data = test_data.copy().drop(['DAY_TYPE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_datatypes(data):\n",
    "    \"\"\"\n",
    "    adjust to appropriate datatypes\n",
    "    \"\"\"\n",
    "    data.TRIP_ID = data.TRIP_ID.astype(object)\n",
    "    data.CALL_TYPE = data.CALL_TYPE.astype(object)\n",
    "    data.ORIGIN_STAND = data.ORIGIN_STAND.astype(object)\n",
    "    data.ORIGIN_CALL = data.ORIGIN_CALL.astype(object)\n",
    "    data.TAXI_ID = data.TAXI_ID.astype(object)\n",
    "    data['TIMESTAMP_DT'] = data['TIMESTAMP'].apply(lambda value_unix: \n",
    "                                                   datetime.fromtimestamp(value_unix).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    data.TIMESTAMP_DT = pd.to_datetime(data.TIMESTAMP_DT)\n",
    "    data['POLYLINE_LIST'] = data['POLYLINE'].apply(lambda value: json.loads(value))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = adjust_datatypes(train_data.copy())\n",
    "test_data = adjust_datatypes(test_data.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1710670 entries, 0 to 1710669\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Dtype         \n",
      "---  ------         -----         \n",
      " 0   TRIP_ID        object        \n",
      " 1   CALL_TYPE      object        \n",
      " 2   ORIGIN_CALL    object        \n",
      " 3   ORIGIN_STAND   object        \n",
      " 4   TAXI_ID        object        \n",
      " 5   TIMESTAMP      int64         \n",
      " 6   MISSING_DATA   bool          \n",
      " 7   POLYLINE       object        \n",
      " 8   WEATHER        object        \n",
      " 9   TIMESTAMP_DT   datetime64[ns]\n",
      " 10  POLYLINE_LIST  object        \n",
      "dtypes: bool(1), datetime64[ns](1), int64(1), object(8)\n",
      "memory usage: 132.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 320 entries, 0 to 319\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   TRIP_ID        320 non-null    object        \n",
      " 1   CALL_TYPE      320 non-null    object        \n",
      " 2   ORIGIN_CALL    72 non-null     object        \n",
      " 3   ORIGIN_STAND   123 non-null    object        \n",
      " 4   TAXI_ID        320 non-null    object        \n",
      " 5   TIMESTAMP      320 non-null    int64         \n",
      " 6   MISSING_DATA   320 non-null    bool          \n",
      " 7   POLYLINE       320 non-null    object        \n",
      " 8   WEATHER        320 non-null    object        \n",
      " 9   TIMESTAMP_DT   320 non-null    datetime64[ns]\n",
      " 10  POLYLINE_LIST  320 non-null    object        \n",
      "dtypes: bool(1), datetime64[ns](1), int64(1), object(8)\n",
      "memory usage: 25.4+ KB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-07-01 00:00:53\n",
      "2014-06-30 23:59:56\n",
      "2014-08-14 16:02:23\n",
      "2014-12-21 14:29:19\n"
     ]
    }
   ],
   "source": [
    "print(train_data.TIMESTAMP_DT.min())\n",
    "print(train_data.TIMESTAMP_DT.max())\n",
    "print(test_data.TIMESTAMP_DT.min())\n",
    "print(test_data.TIMESTAMP_DT.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All dates are in previously defined valid ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assert that train and test data have same column shape and attributes\n",
    "def perform_sanity_checks():\n",
    "    try:\n",
    "        assert(train_data.shape[1] == test_data.shape[1])\n",
    "        print(\"Column shape train vs test passed\")\n",
    "        assert((train_data.columns == test_data.columns).all())\n",
    "        print(\"Column naming train vs test passed\")\n",
    "        assert(train_data.TRIP_ID.nunique() == train_data.shape[0])\n",
    "        print(\"Check for unique trips passed - train data\")\n",
    "        assert(test_data.TRIP_ID.nunique() == test_data.shape[0])\n",
    "        print(\"Check for unique trips passed - test data\")\n",
    "        print('All checks passed!')\n",
    "    except:\n",
    "        print(\"Sanity Check failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column shape train vs test passed\n",
      "Column naming train vs test passed\n",
      "Sanity Check failed\n"
     ]
    }
   ],
   "source": [
    "perform_sanity_checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5 Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.5.1 NAN/Null Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TRIP_ID                0\n",
       "CALL_TYPE              0\n",
       "ORIGIN_CALL      1345900\n",
       "ORIGIN_STAND      904091\n",
       "TAXI_ID                0\n",
       "TIMESTAMP              0\n",
       "MISSING_DATA           0\n",
       "POLYLINE               0\n",
       "WEATHER                0\n",
       "TIMESTAMP_DT           0\n",
       "POLYLINE_LIST          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TRIP_ID            0\n",
       "CALL_TYPE          0\n",
       "ORIGIN_CALL      248\n",
       "ORIGIN_STAND     197\n",
       "TAXI_ID            0\n",
       "TIMESTAMP          0\n",
       "MISSING_DATA       0\n",
       "POLYLINE           0\n",
       "WEATHER            0\n",
       "TIMESTAMP_DT       0\n",
       "POLYLINE_LIST      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ORIGIN_CALL and ORIGIN_STAND have null values which is to be expected as they are determined dependent on the call type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.5.2 Duplicated TRIP_IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sanity Checks in 0.4 showed that the TRIP_IDs are not unique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 TRIP_IDs are duplicated\n",
      "0.0046767516919610725 % out of all unique TRIPs.\n"
     ]
    }
   ],
   "source": [
    "vc = train_data.TRIP_ID.value_counts().reset_index()\n",
    "DUPLICATED_IDs = vc[vc.TRIP_ID > 1]['index'].unique()\n",
    "print(f'{len(DUPLICATED_IDs)} TRIP_IDs are duplicated')\n",
    "print(f'{(len(DUPLICATED_IDs)/train_data.TRIP_ID.nunique()*100)} % out of all unique TRIPs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**:\n",
    "- 159 cases\n",
    "- Missing Data == False for all cases\n",
    "- 80 TRIP_IDs are duplicated\n",
    "- Affected data is insignifcant (less than 1% of all  TRIPs)\n",
    "\n",
    "**Assumptions**:\n",
    "- Potential reasons could be cancellation by dispatcher after a person called for some reasons, failed flight attempts, broken flight taxi etc.\n",
    "- The trips per ID with the longest POLYLINE are kept as these are assumed to be valid trips. Additionally trips with no POLYLINE or only one coordinate point are assumed invalid and filtered from the dataset. \n",
    "- Also it is assumed that only POLYLINEs with at least 5 coordinate points are sufficient. \n",
    "- Further investigation will should be done and analyzed together with sensor/technical data from flight taxi. Also the reason could be that a flight is interrupted and re-started again, that could be analyzed by plotting the POLYLINE and compare the start and end point of the duplicated TRIPs. Will be part of further optimization\n",
    "\n",
    "To do as mentioned in Assumptions, the number of points in the POLYLINE needs to be calculated. In addition we calculate the total flight time at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.5.3 Data Cleaning POLYLINE\n",
    "To do cleaning regarding the POLYLINE, a few more attributes are calculated:\n",
    "- N_COORDINATE_POINTS - number of total points\n",
    "- TOTAL_FLIGHT_TIME_SECONDS, TOTAL_FLIGHT_TIME_MINUTES - flight time total\n",
    "- START_POINT - Starting point for each trip\n",
    "- DEST_POINT - Last point for each trip \n",
    "- TOTAL_DISTANCE - total distance of trip in km with haversine formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_POLYLINE_features(data):\n",
    "    \"\"\"\n",
    "    calculates length of POLYLINE as N_COORDINATE_POINTS\n",
    "    calculates total flight time in seconds and minutes as TOTAL_FLIGHT_TIME_SECONDS and TOTAL_FLIGHT_TIME_MINUTES\n",
    "    \"\"\"\n",
    "    data['N_COORDINATE_POINTS'] = data['POLYLINE_LIST'].apply(lambda value: len(value))\n",
    "    #total flight time\n",
    "    data['TOTAL_FLIGHT_TIME_SECONDS'] = data.apply(lambda row: (row.N_COORDINATE_POINTS-1)*15, axis=1)\n",
    "    data['TOTAL_FLIGHT_TIME_MINUTES'] = data.TOTAL_FLIGHT_TIME_SECONDS / 60\n",
    "    return data\n",
    "\n",
    "def calculate_total_distance(data):\n",
    "    data['START_POINT'] = data['POLYLINE_LIST'].apply(lambda value: value[0])\n",
    "    data['DEST_POINT'] = data['POLYLINE_LIST'].apply(lambda value: value[-1])   \n",
    "    data['TOTAL_DISTANCE_KM'] = data.apply(lambda row:\n",
    "                                       haversine_distance(lat1=row.START_POINT[1],\n",
    "                                                lat2=row.DEST_POINT[1],\n",
    "                                                lon1=row.START_POINT[0],\n",
    "                                                lon2=row.DEST_POINT[0])\n",
    "                                        ,axis=1\n",
    "                                       )\n",
    "    return data\n",
    "\n",
    "def haversine_distance(lat1, lat2, lon1, lon2):\n",
    "    #analog to following source -> https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.haversine_distances.html\n",
    "    from sklearn.metrics.pairwise import haversine_distances\n",
    "    from math import radians\n",
    "    point_A = [lon1, lat1] \n",
    "    point_B = [lon2, lat2]\n",
    "    pointA_in_radians = [radians(_) for _ in point_A]\n",
    "    pointB_in_radians = [radians(_) for _ in point_B]\n",
    "    result = haversine_distances([pointA_in_radians, pointB_in_radians])\n",
    "    result = result * 6371000/1000  \n",
    "    return result[0][1]\n",
    "\n",
    "def filter_invalid_trips(data):\n",
    "    \"\"\"\n",
    "    filters trips with less than 5 coordinate points and takes data sample with longest POLYLINE for duplicated TRIP IDs\n",
    "    \"\"\"\n",
    "    data = data[data.N_COORDINATE_POINTS >= 5]\n",
    "    vc = data.TRIP_ID.value_counts().reset_index()\n",
    "    DUPLICATES_IDs = vc[vc.TRIP_ID > 1]['index'].unique()\n",
    "    if len(DUPLICATES_IDs) > 0:\n",
    "        data_duplicated = data[data.TRIP_ID.isin(DUPLICATES_IDs)]\n",
    "        data_valid = data[~data.TRIP_ID.isin(DUPLICATES_IDs)]\n",
    "        data_filtered = data_duplicated.groupby('TRIP_ID').apply(lambda datachunk: datachunk[datachunk.N_COORDINATE_POINTS == datachunk.N_COORDINATE_POINTS.max()])\n",
    "        data = pd.concat([data_filtered,data_valid],axis=0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = calculate_POLYLINE_features(train_data.copy())\n",
    "test_data = calculate_POLYLINE_features(test_data.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter invalid trips**\n",
    "- The trips per ID with the longest POLYLINE are kept as these are assumed to be valid trips. Additionally trips with no POLYLINE or only one coordinate point are assumed invalid and filtered from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Including invalid trips - Size data samples 1710670\n",
      "Including invalid trips - Size data samples 320\n",
      "Excluding invalid trips -  Size data samples 1658565\n",
      "Excluding invalid trips -  Size data samples 299\n"
     ]
    }
   ],
   "source": [
    "print_sample_size(train_data, 'Including invalid trips -')\n",
    "print_sample_size(test_data, 'Including invalid trips -')\n",
    "train_data_filtered = filter_invalid_trips(train_data.copy())\n",
    "test_data_filtered = filter_invalid_trips(test_data.copy())\n",
    "print_sample_size(train_data_filtered, 'Excluding invalid trips - ')\n",
    "print_sample_size(test_data_filtered, 'Excluding invalid trips - ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_filtered = train_data_filtered.reset_index(drop=True)\n",
    "test_data_filtered = test_data_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_filtered = calculate_total_distance(train_data_filtered.copy())\n",
    "test_data_filtered = calculate_total_distance(test_data_filtered.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.5.4 MISSING_DATA == TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train Data: {train_data_filtered.MISSING_DATA.value_counts()[1]} Trips with MISSING_DATA == True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data_filtered.MISSING_DATA.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_filtered[train_data_filtered.MISSING_DATA == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Amount of data with missing values insignificant compared to total amount of data\n",
    "- Majority of trips is WEATHER == Rainy, however total amount of trips is not significant enought to draw a conclusion/make an assumption\n",
    "- Number of points/length of polyline is in general unequal so there is no indication in that sense how much data is missing, also no information if data is missing at the start/middle or end of POLYLINE \n",
    "\n",
    "Based on these Findings, I would simply drop these values, mainly as their effect is expected to be very little. If the number of data samples would be higher, I would try to impute the missing coordinates in this case with the Nearest Neighbour. However the problem of knowing if the cooordinates are missing in start/middle/end would prevail. In case I find very similar trips through additional logic (difference in n_coordinate_points <= 5 and overall_distance between points < threshold) I could minimize this problem. These tasks could be part of further optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_filtered = train_data_filtered[train_data_filtered.MISSING_DATA != True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.5.5 OUTLIER\n",
    "To handle the outliers, we look at statistical indicators and plot the boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_filtered.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.boxplot(data=train_data_filtered[['N_COORDINATE_POINTS','TOTAL_FLIGHT_TIME_MINUTES','TOTAL_DISTANCE_KM']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cotinous attributes show a high number of outliers, with the number of coordinate points the widest spread.\n",
    "To avoid loosing too much data, keeping the 95% quantile of the data regarding the N_COORDINATE_POINTS and TOTAL_DISTANCE seems to be the best choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_filtered = train_data_filtered[(train_data_filtered.N_COORDINATE_POINTS <= train_data_filtered.N_COORDINATE_POINTS.quantile(0.95))\n",
    "                                   & (train_data_filtered.TOTAL_DISTANCE_KM <= train_data_filtered.TOTAL_DISTANCE_KM.quantile(0.95))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.boxplot(data=train_data_filtered[['N_COORDINATE_POINTS','TOTAL_FLIGHT_TIME_MINUTES','TOTAL_DISTANCE_KM']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some outliers remaining, however the spread is significantly reduced. Outliers in the test data will be kept to avoid too much reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.hist(train_data.TOTAL_FLIGHT_TIME_MINUTES[train_data.TOTAL_FLIGHT_TIME_MINUTES <= train_data.TOTAL_FLIGHT_TIME_MINUTES.quantile(0.95)], \n",
    "         label=f'Pre invalid trips N={train_data.shape[0]}')\n",
    "plt.hist(train_data_filtered.TOTAL_FLIGHT_TIME_MINUTES[train_data_filtered.TOTAL_FLIGHT_TIME_MINUTES<= train_data_filtered.TOTAL_FLIGHT_TIME_MINUTES.quantile(0.95)], \n",
    "         label=f'Post invalid trips N={train_data_filtered.shape[0]}',\n",
    "        alpha=0.5)\n",
    "plt.title('Distribution - total flight time in minutes (95% quantile for visualization reasons)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.hist(train_data_filtered.TOTAL_DISTANCE_KM[train_data_filtered.TOTAL_DISTANCE_KM<= train_data_filtered.TOTAL_DISTANCE_KM.quantile(0.95)], \n",
    "         label=f'Post invalid trips N={train_data_filtered.shape[0]}',\n",
    "        alpha=0.5)\n",
    "plt.title('Distribution - Count total distance (95% quantile for visualization reasons)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.hist(train_data.N_COORDINATE_POINTS[train_data.N_COORDINATE_POINTS <= train_data.N_COORDINATE_POINTS.quantile(0.95)], \n",
    "         label=f'Pre invalid trips N={train_data.shape[0]}')\n",
    "plt.hist(train_data_filtered.N_COORDINATE_POINTS[train_data_filtered.N_COORDINATE_POINTS<= train_data_filtered.N_COORDINATE_POINTS.quantile(0.95)], \n",
    "         label=f'Post invalid trips N={train_data_filtered.shape[0]}',\n",
    "        alpha=0.5)\n",
    "plt.title('Distribution - Count coordinate points (95% quantile for visualization reasons)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduction of the training data does not have a major effect on the data distribution. Optimization could be to compare performance with/without outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data_filtered.reset_index(drop=True)\n",
    "test_data = test_data_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_sanity_checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.TRIP_ID.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[train_data.TRIP_ID == 1397172149620000454].POLYLINE_LIST.explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[train_data.TRIP_ID == 1397172149620000454]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.5.5 CALL_TYPE LOGIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_call_type(data):\n",
    "    data_A = data[(data.CALL_TYPE == 'A') & (data.ORIGIN_CALL == np.NaN)]\n",
    "    data_B = data[(data.CALL_TYPE == 'B') & (data.ORIGIN_STAND == np.NaN)]\n",
    "    data_C = data[(data.CALL_TYPE == 'C') & (data.ORIGIN_STAND != np.NaN)].ORIGIN_STAND.nunique()\n",
    "    return data_A, data_B, data_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_call_type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_call_type(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Exploratory Data Analysis  & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = train_data.corr()\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.groupby('CALL_TYPE', as_index=False)[['TOTAL_FLIGHT_TIME_MINUTES', 'TOTAL_DISTANCE_KM']].agg(['mean', 'median','std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby('WEATHER', as_index=False)[['TOTAL_FLIGHT_TIME_MINUTES', 'TOTAL_DISTANCE_KM']].agg(['mean', 'median','std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.groupby('WEATHER', as_index=False)['TRIP_ID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No correlations in the data which are not obvious\n",
    "- Especially surprising that the weather is not affecting the total flight time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_freq = train_data.groupby('TAXI_ID', as_index=False)['TRIP_ID'].nunique().sort_values('TRIP_ID',ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_freq.TAXI_ID = trip_freq.TAXI_ID.astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_origin_stand = train_data.ORIGIN_STAND.value_counts(normalize=True).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_origin_stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.figure(figsize=(15,3))\n",
    "plt.bar(vc_origin_stand.index,vc_origin_stand.values, label='TRIPs per ORIGIN STAND')\n",
    "plt.xticks(vc_origin_stand.index, rotation=45)\n",
    "plt.title(f'TRIPs per ORIGIN STAND N={vc_origin_stand.shape[0]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several ORIGIN STAND have high demands, the high cardinality of the feature should be reduced by summarizing ORIGIN STANDS with few demands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_timestamps(data):\n",
    "    data['TIMESTAMP_MONTH'] = pd.to_datetime(data['TIMESTAMP_DT']).dt.month\n",
    "    data['TIMESTAMP_DAY'] = pd.to_datetime(data['TIMESTAMP_DT']).dt.day\n",
    "    data['TIMESTAMP_WEEK'] = pd.to_datetime(data['TIMESTAMP_DT']).dt.isocalendar().week\n",
    "    data['TIMESTAMP_YEAR'] = pd.to_datetime(data['TIMESTAMP_DT']).dt.year\n",
    "    data['YEAR_MONTH'] = data['TIMESTAMP_YEAR'].astype(str) + '_' + data['TIMESTAMP_MONTH'].astype(str)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = extend_timestamps(train_data.copy())\n",
    "test_data = extend_timestamps(test_data.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_year_month = train_data.YEAR_MONTH.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.bar(vc_year_month.index, vc_year_month.values,label='Unique TRIPs per Month')\n",
    "plt.xticks(vc_year_month.index, rotation=45)\n",
    "plt.title(f'TRIPs per MONTH N={vc_year_month.shape[0]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Feature Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CALL TYPE -> ONE_HOT ENCODING, no ordinal relationship\n",
    "- WEATHER --> ONE HOT ENCODING, no ordinal relationship\n",
    "- ORIGIN STAND --> Reduction of High cardinality + ONE HOT ENCODING\n",
    "- MONTH/WEEK per year --> ONE HOT ENCODING or ORDINAL ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_high_cardinality(data, columns=[]):\n",
    "    for column_ in columns:\n",
    "        vc = data[str(column_)].value_counts()\n",
    "        median_freq = vc.median()\n",
    "        majority_freq = vc[vc >= median_freq].index\n",
    "        data[str(column_)+ '_agg'] = data.apply(lambda row: row[str(column_)] if row[str(column_)] in majority_freq else 'OTHER', axis=1)\n",
    "        data[str(column_)+ '_agg'] = data[str(column_)+ '_agg'].astype(str)\n",
    "    return data\n",
    "\n",
    "def feature_encoding_oh(data, categories_oh):\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    data_encoded = pd.DataFrame()\n",
    "    for attribute_ in categories_oh:\n",
    "        enc = OneHotEncoder()\n",
    "        fenc = enc.fit_transform(X=data[str(attribute_)].values.reshape(-1,1)).toarray()\n",
    "        df_fenc = pd.DataFrame(fenc, columns=enc.categories_)\n",
    "        data_encoded = pd.concat([df_fenc, data_encoded], axis=1)\n",
    "    return data_encoded\n",
    "        \n",
    "\n",
    "def feature_encoding_oe(data, categories_oe):\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    data_encoded = pd.DataFrame()\n",
    "    for attribute_ in categories_oe:\n",
    "        enc = OrdinalEncoder()\n",
    "        fenc = enc.fit_transform(X=data[str(attribute_)].values.reshape(-1,1))\n",
    "        df_fenc = pd.DataFrame(fenc, columns=[str(attribute_)+'_OE'])\n",
    "        data_encoded = pd.concat([df_fenc, data_encoded], axis=1)\n",
    "    return data_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = reduce_high_cardinality(train_data, ['ORIGIN_STAND'])\n",
    "test_data = reduce_high_cardinality(test_data, ['ORIGIN_STAND'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_oh = ['CALL_TYPE','WEATHER','ORIGIN_STAND_agg']\n",
    "categories_oe = ['YEAR_MONTH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fenc_oh = feature_encoding_oh(train_data, categories_oh)\n",
    "df_fenc_oe = feature_encoding_oe(train_data, categories_oe)\n",
    "train_data = pd.concat([train_data, df_fenc_oh, df_fenc_oe],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fenc_oh = feature_encoding_oh(test_data, categories_oh)\n",
    "df_fenc_oe = feature_encoding_oe(test_data, categories_oe)\n",
    "test_data = pd.concat([test_data, df_fenc_oh, df_fenc_oe],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_binary_features(train_data, test_data):\n",
    "    missing_features_test = train_data.columns.difference(test_data.columns)\n",
    "    missing_features_train = test_data.columns.difference(train_data.columns)\n",
    "    zero_data_test = np.zeros(shape=(test_data.shape[0],len(missing_features_test)))\n",
    "    dummy_data_test = pd.DataFrame(zero_data_test, columns=missing_features_test)\n",
    "    zero_data_train = np.zeros(shape=(train_data.shape[0],len(missing_features_train))) \n",
    "    dummy_data_train = pd.DataFrame(zero_data_train, columns=missing_features_train)\n",
    "    \n",
    "    return pd.concat([test_data, dummy_data_test],axis=1), pd.concat([train_data, dummy_data_train],axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, train_data = add_binary_features(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(train_data.shape[1] == test_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_features = ['CALL_TYPE','ORIGIN_CALL','ORIGIN_STAND', 'TOTAL_FLIGHT_TIME_SECONDS', 'START_POINT','DEST_POINT',\n",
    "                'TIMESTAMP_MONTH','TIMESTAMP_DAY','TIMESTAMP_WEEK','TIMESTAMP_YEAR','YEAR_MONTH','ORIGIN_STAND_agg',\n",
    "               'MISSING_DATA','POLYLINE','WEATHER','TAXI_ID','TIMESTAMP_DT','TIMESTAMP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[['DEST_LON','DEST_LAT']] = pd.DataFrame(train_data.DEST_POINT.tolist(), columns=['DEST_LON','DEST_LAT'])\n",
    "test_data[['DEST_LON','DEST_LAT']] = pd.DataFrame(test_data.DEST_POINT.tolist(), columns=['DEST_LON','DEST_LAT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[[column_ for column_ in train_data.columns if column_ not in non_features]].to_csv('s3://think-tank-casestudy/features_engineered/feature_engineered_regress_train.csv')\n",
    "test_data[[column_ for column_ in train_data.columns if column_ not in non_features]].to_csv('s3://think-tank-casestudy/features_engineered/feature_engineered_regress_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Multi-regression output problem --> predict two labels: longitude and latitude\n",
    "\n",
    "2) Re-write problem as multi-class problem through clustering\n",
    "To define the classes, the last points per trip from the training data will be divided into clusters. When we have a large enough number of clusters, the clusters should be small enough to have their points close to their centroids. \n",
    "\n",
    "K-Means is a good approach as it minimizes the sum of squared distances within the cluster. Also it scales well to a large number of clusters. \n",
    "\n",
    "So the approach then predicts: **Probability of final destination of a trip being located in a specific cluster**\n",
    "\n",
    "Steps:\n",
    "- Get optimal number of clusters with help of interia\n",
    "- Generate clusters with K-Means for each trip in training data\n",
    "- Predict cluster for each trip in testing data\n",
    "- Get centroids for each cluster \n",
    "- Calculate distance from points in cluster to centroid to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##takes too long to execute\n",
    "\"\"\"\n",
    "from sklearn.cluster import KMeans\n",
    "k_clusters = range(100,2000,100)\n",
    "kmeans = [KMeans(n_clusters=n) for n in k_clusters]\n",
    "X = train_data[['LAST_POINT_LONG','LAST_POINT_LAT']]\n",
    "score = [kmeans[i].fit(X).score(X) for i in range(len(kmeans))]\n",
    "plt.plot(k_clusters, score)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Interia')\n",
    "plt.title('Elbow Curve 100 to 2000 clusters ')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "dest_lon_lat_scaled_train = scaler.fit_transform(train_data[['DEST_LON','DEST_LAT']])\n",
    "dest_lon_lat_scaled_test = scaler.fit_transform(test_data[['DEST_LON','DEST_LAT']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=1000)\n",
    "kmeans.fit(dest_lon_lat_scaled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['cluster_label'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['cluster_label'] = kmeans.predict(dest_lon_lat_scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data = pd.DataFrame(kmeans.cluster_centers_, columns=['center_longitude','center_latitude'])\\\n",
    ".reset_index().rename(columns={'index':'cluster_label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.merge(train_data, cluster_data, how='left', on='cluster_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[[column_ for column_ in train_data.columns if column_ not in non_features]].to_csv('s3://think-tank-casestudy/features_engineered/feature_engineered_class_train.csv')\n",
    "test_data[[column_ for column_ in train_data.columns if column_ not in non_features]].to_csv('s3://think-tank-casestudy/features_engineered/feature_engineered_class_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
